{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO: Implement the fit method\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: Implement the predict method\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = self.compute_distance(x, self.X_train)\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            prob_class_1 = np.sum(k_nearest_labels == 1) / self.k\n",
    "            predictions.append(prob_class_1)  # Returning the probability for class 1\n",
    "        \n",
    "        return np.array(predictions)  # Array of probabilities\n",
    "\n",
    "   \n",
    "\n",
    "    def compute_distance(self, X1, X2):\n",
    "        # TODO: Implement distance computation based on self.distance_metric\n",
    "        # Hint: Use numpy operations for efficient computation\n",
    "        X1 = np.atleast_2d(X1).astype(np.float64)\n",
    "        X2 = np.atleast_2d(X2).astype(np.float64)\n",
    "\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((X2 - X1) ** 2, axis=1))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(X2 - X1), axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data preprocessing function\n",
    "def preprocess_data(train_path, test_path):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    # TODO: Implement data preprocessing\n",
    "    # Handle categorical variables, scale features, etc.\n",
    "    y_train = train_data['Exited']\n",
    "    \n",
    "    X_train = train_data.drop(columns=['Exited', 'id', 'CustomerId', 'Surname'])\n",
    "    test_data = test_data.drop(columns=['id', 'CustomerId', 'Surname'])\n",
    "\n",
    "    # Handle missing values only for numeric columns\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    X_train[numeric_cols] = X_train[numeric_cols].fillna(X_train[numeric_cols].mean())\n",
    "    test_data[numeric_cols] = test_data[numeric_cols].fillna(test_data[numeric_cols].mean())\n",
    "    \n",
    "    # Combine X_train and test_data for consistent preprocessing\n",
    "    full_data = pd.concat([X_train, test_data], axis=0)\n",
    "    \n",
    "    # Handle categorical variables (One-hot encode 'Geography' and 'Gender')\n",
    "    full_data = pd.get_dummies(full_data, columns=['Geography', 'Gender'], drop_first=True)\n",
    "    \n",
    "    # Ensure there are no remaining non-numeric columns\n",
    "    non_numeric_columns = full_data.select_dtypes(include=[object]).columns\n",
    "    assert non_numeric_columns.empty, f\"There are still non-numeric columns: {non_numeric_columns}\"\n",
    "    \n",
    "    # Manually scale numerical features using mean and standard deviation\n",
    "    numeric_cols = full_data.select_dtypes(include=[np.number]).columns\n",
    "    means = full_data[numeric_cols].mean()\n",
    "    stds = full_data[numeric_cols].std()\n",
    "    full_data[numeric_cols] = (full_data[numeric_cols] - means) / stds\n",
    "    \n",
    "    # Ensure no NaN values exist after preprocessing\n",
    "    assert not full_data.isnull().values.any(), \"There are NaN values in the dataset!\"\n",
    "    \n",
    "    # Separate the preprocessed train and test data\n",
    "    X_train = full_data.iloc[:len(train_data), :]\n",
    "    X_test = full_data.iloc[len(train_data):, :]\n",
    "    \n",
    "    return X_train.values, y_train.values, X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation function\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    # TODO: Implement cross-validation\n",
    "    # Compute ROC AUC scores\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Shuffle the data before splitting into folds\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    # Determine the size of each fold\n",
    "    fold_size = len(X) // n_splits\n",
    "    \n",
    "    # Initialize an array to store the AUC scores\n",
    "    auc_scores = []\n",
    "    \n",
    "    # Loop through each fold\n",
    "    for fold in range(n_splits):\n",
    "        # Determine the start and end of the test indices for the current fold\n",
    "        test_start = fold * fold_size\n",
    "        test_end = test_start + fold_size if fold != n_splits - 1 else len(X)\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_test = X[test_start:test_end]\n",
    "        y_test = y[test_start:test_end]\n",
    "        \n",
    "        X_train = np.concatenate([X[:test_start], X[test_end:]], axis=0)\n",
    "        y_train = np.concatenate([y[:test_start], y[test_end:]], axis=0)\n",
    "        \n",
    "        # Fit the KNN model on the training data\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict the labels (for KNN we use class predictions directly)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        \n",
    "        # Compute ROC AUC score for the current fold manually\n",
    "        auc = compute_roc_auc(y_test, y_pred)\n",
    "        auc_scores.append(auc)\n",
    "    \n",
    "    # Return the AUC scores as a numpy array\n",
    "    return np.array(auc_scores)\n",
    "\n",
    "\n",
    "def compute_roc_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the ROC AUC score manually using numpy.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true: numpy array of true labels (0 or 1)\n",
    "    y_pred: numpy array of predicted scores or labels (0 or 1)\n",
    "    \n",
    "    Returns:\n",
    "    roc_auc: float, the computed ROC AUC score\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Sort by predicted values (descending order)\n",
    "    sorted_indices = np.argsort(y_pred)[::-1]\n",
    "    y_true = y_true[sorted_indices]\n",
    "\n",
    "    # Compute true positive rate (TPR) and false positive rate (FPR)\n",
    "    n_pos = np.sum(y_true == 1)\n",
    "    n_neg = np.sum(y_true == 0)\n",
    "    \n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return 0.5  # Undefined AUC when no positive or negative class exists\n",
    "    \n",
    "    tpr = np.cumsum(y_true) / n_pos  # True positive rate\n",
    "    fpr = np.cumsum(1 - y_true) / n_neg  # False positive rate\n",
    "\n",
    "    # Compute AUC as the area under the ROC curve\n",
    "    roc_auc = np.trapz(tpr, fpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.8621671  0.86634166 0.88624942 0.88135714 0.89269228]\n",
      "Best k: 11, Best distance metric: manhattan, Best score: 0.9003462062883862\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('CS506 Customer Churn KNN/train.csv', 'CS506 Customer Churn KNN/test.csv')\n",
    "\n",
    "# Create and evaluate model\n",
    "knn = KNN(k=5, distance_metric='euclidean')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_validate(X, y, knn)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# TODO: hyperparamters tuning\n",
    "def grid_search_knn(X_train, y_train, k_values, distance_metrics):\n",
    "    best_k = None\n",
    "    best_metric = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for k in k_values:\n",
    "        for metric in distance_metrics:\n",
    "            knn = KNN(k=k, distance_metric=metric)\n",
    "            scores = cross_validate(X_train, y_train, knn)\n",
    "            mean_score = np.mean(scores)\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_k = k\n",
    "                best_metric = metric\n",
    "                \n",
    "    print(f\"Best k: {best_k}, Best distance metric: {best_metric}, Best score: {best_score}\")\n",
    "    return best_k, best_metric\n",
    "\n",
    "k_values = [3, 5, 7, 9, 11]\n",
    "distance_metrics = ['euclidean', 'manhattan']\n",
    "\n",
    "# Perform grid search\n",
    "best_k, best_metric = grid_search_knn(X, y, k_values, distance_metrics)\n",
    "\n",
    "\n",
    "# TODO: Train on full dataset with optimal hyperparameters and make predictions on test set\n",
    "knn = KNN(k=best_k, distance_metric=best_metric)\n",
    "knn.fit(X, y)\n",
    "test_predictions = knn.predict(X_test)\n",
    "\n",
    "# Save test predictions\n",
    "pd.DataFrame({'id': pd.read_csv('CS506 Customer Churn KNN/test.csv')['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost KNN #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostKNN:\n",
    "    def __init__(self, base_learner, n_estimators=50):\n",
    "        self.base_learner = base_learner\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.learners = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize weights equally\n",
    "        n_samples = len(y)\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Train a new weak learner (KNN) with the current weights\n",
    "            learner = self.base_learner()\n",
    "            learner.fit(X, y)\n",
    "            y_pred = learner.predict(X)\n",
    "            \n",
    "            # Compute weighted error rate\n",
    "            misclassified = (y_pred != y)\n",
    "            error = np.sum(weights * misclassified) / np.sum(weights)\n",
    "\n",
    "            # Compute alpha (classifier's contribution to the final model)\n",
    "            alpha = 0.5 * np.log((1 - error) / (error + 1e-10))  # Add small constant to avoid division by zero\n",
    "\n",
    "            # Update weights: increase for misclassified samples\n",
    "            weights *= np.exp(-alpha * y * (2 * y_pred - 1))  # y * (-1 or 1) for misclassification\n",
    "            \n",
    "            # Normalize weights\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "            # Store this learner and its alpha\n",
    "            self.learners.append(learner)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Initialize predictions to zero\n",
    "        final_predictions = np.zeros(X.shape[0])\n",
    "\n",
    "        for alpha, learner in zip(self.alphas, self.learners):\n",
    "            y_pred = learner.predict(X)\n",
    "            final_predictions += alpha * (2 * y_pred - 1)  # Use (2 * y_pred - 1) to map {0, 1} -> {-1, 1}\n",
    "\n",
    "        # Return final predictions\n",
    "        return np.where(final_predictions >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_adaboost_knn(X_train, y_train, k_values, n_estimators_values, distance_metrics):\n",
    "    best_k = None\n",
    "    best_metric = None\n",
    "    best_n_estimators = None\n",
    "    best_score = 0\n",
    "    \n",
    "    # Iterate over all combinations of k, distance metrics, and n_estimators\n",
    "    for k in k_values:\n",
    "        for metric in distance_metrics:\n",
    "            for n_estimators in n_estimators_values:\n",
    "                # Initialize AdaBoost with KNN as the base learner\n",
    "                ada_knn = AdaBoostKNN(base_learner=lambda: KNN(k=k, distance_metric=metric), n_estimators=n_estimators)\n",
    "                \n",
    "                # Perform cross-validation and compute the mean score\n",
    "                scores = cross_validate(X_train, y_train, ada_knn)\n",
    "                mean_score = np.mean(scores)\n",
    "                \n",
    "                # Update the best parameters if current score is better\n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_k = k\n",
    "                    best_metric = metric\n",
    "                    best_n_estimators = n_estimators\n",
    "    \n",
    "    # Print the best combination of parameters\n",
    "    print(f\"Best k: {best_k}, Best distance metric: {best_metric}, Best n_estimators: {best_n_estimators}, Best score: {best_score}\")\n",
    "    \n",
    "    return best_k, best_metric, best_n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test = preprocess_data('CS506 Customer Churn KNN/train.csv', 'CS506 Customer Churn KNN/test.csv')\n",
    "\n",
    "k_values = [3, 5, 7, 9, 11]\n",
    "n_estimators_values = [10, 50, 100]\n",
    "distance_metrics = ['euclidean', 'manhattan']\n",
    "\n",
    "best_k, best_metric, best_n_estimators = grid_search_adaboost_knn(X, y, k_values, n_estimators_values, distance_metrics)\n",
    "\n",
    "# Train the model on the full dataset with optimal hyperparameters\n",
    "ada_knn = AdaBoostKNN(base_learner=lambda: KNN(k=best_k, distance_metric=best_metric), n_estimators=best_n_estimators)\n",
    "ada_knn.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = ada_knn.predict(X_test)\n",
    "\n",
    "# Save the test predictions\n",
    "pd.DataFrame({'id': pd.read_csv('CS506 Customer Churn KNN/test.csv')['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
